{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "## AdaBoost: Grid Search Hyperparameters\n",
    "* In this case, we will grid search two key hyperparameters for AdaBoost: **the number of trees used in the ensemble and the learning rate**. We will use a range of popular well performing values for each hyperparameter. Each configuration combination will be evaluated using repeated k-fold cross-validation and configurations will be compared using the mean score, in this case, classification accuracy. The complete example of grid searching the key hyperparameters of the AdaBoost algorithm on our synthetic classification dataset is listed below.\n",
    "* Repeated k-fold cross-validation provides a way to improve the estimated performance of a machine learning model. This involves simply repeating the cross-validation procedure multiple times and reporting the mean result across all folds from all runs. This mean result is expected to be a more accurate estimate of the true unknown underlying mean performance of the model on the dataset, as calculated using the standard error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.813667 using {'learning_rate': 0.1, 'n_estimators': 500}\n",
      "0.646333 (0.036376) with: {'learning_rate': 0.0001, 'n_estimators': 10}\n",
      "0.646667 (0.036545) with: {'learning_rate': 0.0001, 'n_estimators': 50}\n",
      "0.646667 (0.036545) with: {'learning_rate': 0.0001, 'n_estimators': 100}\n",
      "0.647000 (0.038136) with: {'learning_rate': 0.0001, 'n_estimators': 500}\n",
      "0.646667 (0.036545) with: {'learning_rate': 0.001, 'n_estimators': 10}\n",
      "0.647000 (0.038136) with: {'learning_rate': 0.001, 'n_estimators': 50}\n",
      "0.654333 (0.045511) with: {'learning_rate': 0.001, 'n_estimators': 100}\n",
      "0.672667 (0.046543) with: {'learning_rate': 0.001, 'n_estimators': 500}\n",
      "0.648333 (0.042197) with: {'learning_rate': 0.01, 'n_estimators': 10}\n",
      "0.671667 (0.045613) with: {'learning_rate': 0.01, 'n_estimators': 50}\n",
      "0.715000 (0.053213) with: {'learning_rate': 0.01, 'n_estimators': 100}\n",
      "0.767667 (0.045948) with: {'learning_rate': 0.01, 'n_estimators': 500}\n",
      "0.716667 (0.048876) with: {'learning_rate': 0.1, 'n_estimators': 10}\n",
      "0.767000 (0.049271) with: {'learning_rate': 0.1, 'n_estimators': 50}\n",
      "0.784667 (0.042874) with: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "0.813667 (0.032092) with: {'learning_rate': 0.1, 'n_estimators': 500}\n",
      "0.773333 (0.038759) with: {'learning_rate': 1.0, 'n_estimators': 10}\n",
      "0.806333 (0.040701) with: {'learning_rate': 1.0, 'n_estimators': 50}\n",
      "0.801000 (0.032491) with: {'learning_rate': 1.0, 'n_estimators': 100}\n",
      "0.792667 (0.027801) with: {'learning_rate': 1.0, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# example of grid searching key hyperparameters for adaboost on a classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "    random_state=6)\n",
    "# define the model with default hyperparameters\n",
    "model = AdaBoostClassifier()\n",
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100, 500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,\n",
    "scoring='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) # summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this case, we can see that a configuration with 500 trees and a learning rate of 0.1 performed the best with a classification accuracy of about 81.3 percent. The model may perform even better with more trees such as 1,000 or 5,000 although these configurations were not tested in this case to ensure that the grid search completed in a reasonable time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting: Grid Search Hyperparameters\n",
    "* Gradient boosting can be challenging to configure as the algorithm has many key hyperparameters that influence the behavior of the model on training data and the hyperparameters interact with each other. As such, it is a good practice to use a search process to discover a configuration of the model hyperparameters that works well or best for a given predictive modeling problem. Popular search processes include a random search and a grid search. In this section we will look at grid searching common ranges for the key hyperparameters for the gradient boosting algorithm that you can use as starting point for your own projects. This can be achieving using the GridSearchCV class and specifying a dictionary that maps model hyperparameter names to the values to search.\n",
    "* In this case, we will grid search four key hyperparameters for gradient boosting: **the number of trees used in the ensemble, the learning rate, subsample size used to train each tree, and the maximum depth of each tree**. We will use a range of popular well performing values for each hyperparameter. Each configuration combination will be evaluated using repeated k-fold cross-validation and configurations will be compared using the mean score, in this case, classification accuracy. The complete example of grid searching the key hyperparameters of the gradient boosting algorithm on our synthetic classification dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.945000 using {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.527333 (0.082014) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.525667 (0.078769) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.524000 (0.072874) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.773000 (0.035228) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.766667 (0.039693) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.738667 (0.049982) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.825000 (0.033040) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.817000 (0.035133) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.761000 (0.043077) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.832000 (0.039531) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.812333 (0.042323) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.773667 (0.034975) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.535667 (0.107135) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.536333 (0.109071) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.532333 (0.097423) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.834000 (0.027031) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.845000 (0.031172) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.805667 (0.032525) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.874000 (0.033126) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.868000 (0.029371) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.807333 (0.028628) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.888000 (0.030155) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.879000 (0.030150) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.811000 (0.032491) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.536000 (0.108031) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.536000 (0.108031) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.533333 (0.100078) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.841333 (0.035845) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.844667 (0.030412) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.808333 (0.030341) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.875333 (0.031170) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.880667 (0.028276) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.812333 (0.025908) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.892333 (0.027408) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.883333 (0.028324) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.811667 (0.027090) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.811667 (0.038651) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.809667 (0.038252) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.761333 (0.043107) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.828000 (0.038070) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.816000 (0.040546) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.773667 (0.034975) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.831667 (0.042979) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.811667 (0.042433) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.781333 (0.034325) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.845667 (0.033435) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.831333 (0.037836) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.814333 (0.032730) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.850333 (0.030603) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.855333 (0.029859) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.805000 (0.031807) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.887667 (0.028831) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.875000 (0.026552) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.809667 (0.032299) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.886333 (0.033415) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.874333 (0.027771) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.811667 (0.031946) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.897333 (0.028860) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.883667 (0.030494) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.822333 (0.023900) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.858000 (0.031665) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.855000 (0.031596) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.812000 (0.027374) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.889333 (0.033954) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.879000 (0.021962) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.813333 (0.028674) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.892333 (0.030950) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.882667 (0.030543) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.813000 (0.030017) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.903000 (0.027343) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.892333 (0.029853) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.819333 (0.028040) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.823667 (0.033013) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.813000 (0.040509) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.780000 (0.035214) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.847667 (0.032730) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.833333 (0.034960) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.813333 (0.032283) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.854333 (0.027041) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.851333 (0.032428) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.836000 (0.034020) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.896333 (0.032402) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.892333 (0.029061) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.878000 (0.029484) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.868333 (0.029335) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.869667 (0.032607) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.810333 (0.033910) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.889000 (0.032078) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.879000 (0.028325) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.822333 (0.023192) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.897000 (0.028184) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.890333 (0.030603) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.837000 (0.025968) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.924333 (0.028946) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.919333 (0.029769) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.886667 (0.030587) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.864667 (0.034033) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.867000 (0.032368) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.814000 (0.027398) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.891667 (0.030341) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.888333 (0.027090) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.818333 (0.029107) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.903333 (0.029250) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.896000 (0.030177) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.818000 (0.028213) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.924667 (0.026800) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.920333 (0.029267) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.831667 (0.026594) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.846000 (0.034215) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.837667 (0.026543) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.830000 (0.036515) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.890667 (0.028511) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.886000 (0.029166) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.880000 (0.034351) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.902667 (0.033460) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.907667 (0.025124) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.898333 (0.030120) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.923667 (0.023019) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.928000 (0.023007) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.918000 (0.025087) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.868000 (0.036733) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.877000 (0.026727) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.832000 (0.026382) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.913000 (0.027465) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.909333 (0.028975) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.887667 (0.032933) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.926333 (0.028341) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.930333 (0.027262) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.905667 (0.032216) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.942667 (0.022201) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.945000 (0.023058) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.927333 (0.026949) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.870000 (0.030221) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.877000 (0.029343) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.823667 (0.027747) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.914667 (0.026550) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.910667 (0.028394) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.840333 (0.031779) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.929000 (0.025080) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.928000 (0.026128) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.835667 (0.041043) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.941000 (0.027731) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.940333 (0.024424) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.859333 (0.044492) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.828333 (0.044951) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.850333 (0.041108) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.848000 (0.029257) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.837000 (0.039085) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.871333 (0.029295) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.897667 (0.032320) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.823667 (0.037102) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.874000 (0.035926) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.904000 (0.032104) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.811667 (0.064036) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.897000 (0.036162) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.925000 (0.028954) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.814333 (0.042558) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.838333 (0.040007) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.879333 (0.028628) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.836000 (0.044692) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.903000 (0.028184) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.918667 (0.031595) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.824667 (0.052328) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.912333 (0.038874) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.917000 (0.027946) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.725333 (0.144677) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.913333 (0.025078) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.916667 (0.032387) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.799333 (0.037765) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.848333 (0.032872) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.887000 (0.031107) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.847333 (0.033658) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.912667 (0.031930) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.912333 (0.028010) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.797333 (0.109937) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.912333 (0.028482) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.914000 (0.023180) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.793667 (0.127292) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.922667 (0.029318) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.913333 (0.028790) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 500, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# example of grid searching key hyperparameters for gradient boosting\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "    random_state=7)\n",
    "# define the model with default hyperparameters\n",
    "model = GradientBoostingClassifier()\n",
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100, 500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "grid['subsample'] = [0.5, 0.7, 1.0]\n",
    "grid['max_depth'] = [3, 7, 9]\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,\n",
    "scoring='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) # summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this case, we can see that a configuration with a learning rate of 0.1, max depth of 7 levels, 500 trees and a subsample of 70% performed the best with a classification accuracy of about 94.5 percent. The model may perform even better with more trees such as 1,000 or 5,000 although these configurations were not tested in this case to ensure that the grid search completed in a reasonable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
