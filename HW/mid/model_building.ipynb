{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import those package we need \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm import tqdm  # 引入 tqdm\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, auc, f1_score, accuracy_score, classification_report, confusion_matrix, roc_auc_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train_clean.csv\") \n",
    "X_test = pd.read_csv(\"X_test_clean.csv\")\n",
    "y_train = pd.read_csv(\"y_train_clean.csv\")\n",
    "y_test = pd.read_csv(\"y_test_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(dtype = 'int')\n",
    "y_test = y_test.astype(dtype = 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pickle\n",
    "\n",
    "# # LightGBM\n",
    "\n",
    "# lgb_train = lgb.Dataset(X_train, label = y_train)\n",
    "# lgb_eval = lgb.Dataset(X_test, label = y_test, reference = lgb_train)\n",
    "# params = {\n",
    "#     'objective': 'binary',\n",
    "#     'metric': 'binary_logloss',\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'num_leaves': 31,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'feature_fraction': 0.9,\n",
    "#     'bagging_fraction': 0.8,\n",
    "#     'bagging_freq': 5,\n",
    "#     'max_depth': 10\n",
    "# }\n",
    "\n",
    "# num_round = 3000\n",
    "# bst = lgb.train(params, lgb_train, num_round, \n",
    "#                 valid_sets = [lgb_train, lgb_eval], \n",
    "#                 callbacks = [lgb.early_stopping(stopping_rounds = 200)]\n",
    "#                 )\n",
    "\n",
    "# # 保存模型 - 方法1：使用 LightGBM 的 save_model 方法\n",
    "# bst.save_model('lightgbm_model.txt')\n",
    "\n",
    "# # 保存模型 - 方法2：使用 pickle\n",
    "# with open('lightgbm_model.pkl', 'wb') as f:\n",
    "#     pickle.dump(bst, f)\n",
    "\n",
    "# # Light GBM 特徵重要性\n",
    "# feature_importance_in_lgbm = bst.feature_importance(importance_type = 'split')  # 或 'gain'，取決於你想要的類型\n",
    "\n",
    "# feature_names_in_lgbm = bst.feature_name()\n",
    "\n",
    "# feature_importance_df_lgbm = pd.DataFrame({'Feature': feature_names_in_lgbm, \n",
    "#                                            'Importance': feature_importance_in_lgbm}\n",
    "#                                            )\n",
    "\n",
    "# # 依照特徵重要性降序排序\n",
    "# feature_importance_df_lgbm = feature_importance_df_lgbm.sort_values(by = 'Importance', ascending = False)\n",
    "\n",
    "# # 繪製特徵重要性圖表\n",
    "# plt.figure(figsize = (10, 6))\n",
    "# plt.barh(feature_importance_df_lgbm['Feature'], \n",
    "#          feature_importance_df_lgbm['Importance']\n",
    "#          )\n",
    "\n",
    "# plt.xlabel('Importance')\n",
    "# plt.title('Feature Importance in lgbm')\n",
    "# plt.show()\n",
    "\n",
    "# # 保存特徵重要性數據\n",
    "# feature_importance_df_lgbm.to_csv('lightgbm_feature_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, label = y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, label = y_test, reference = lgb_train)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'max_depth': 10\n",
    "}\n",
    "\n",
    "num_round = 3000\n",
    "#bst = lgb.train(params, lgb_train, num_round, valid_sets=[lgb_train, lgb_eval])\n",
    "bst = lgb.train(params, lgb_train, num_round, \n",
    "                valid_sets = [lgb_train, lgb_eval], \n",
    "                callbacks = [lgb.early_stopping(stopping_rounds = 200)]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light GBM \n",
    "feature_importance_in_lgbm = bst.feature_importance(importance_type = 'split')  # 或 'gain'，取決於你想要的類型\n",
    "\n",
    "# 獲取特徵名稱\n",
    "feature_names_in_lgbm = bst.feature_name()\n",
    "\n",
    "# 將特徵重要性轉換為 DataFrame\n",
    "feature_importance_df_lgbm = pd.DataFrame({'Feature': feature_names_in_lgbm, \n",
    "                                           'Importance': feature_importance_in_lgbm}\n",
    "                                           )\n",
    "\n",
    "# 依照特徵重要性降序排序\n",
    "feature_importance_df_lgbm = feature_importance_df_lgbm.sort_values(by = 'Importance', ascending = False)\n",
    "\n",
    "# 繪製特徵重要性圖表\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.barh(feature_importance_df_lgbm['Feature'], \n",
    "         feature_importance_df_lgbm['Importance']\n",
    "         )\n",
    "\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance in lgbm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell generated by Data Wrangler.\n",
    "\"\"\"\n",
    "def clean_data(X_train):\n",
    "    # Replace missing values with the mean of each column in: 'csmam'\n",
    "    X_train = X_train.fillna({'csmam': X_train['csmam'].mean()})\n",
    "    # Replace gaps forward from the previous valid value in: 'flg_3dsmk'\n",
    "    X_train = X_train.fillna({'flg_3dsmk': X_train['flg_3dsmk'].ffill()})\n",
    "    return X_train\n",
    "\n",
    "X_train = clean_data(X_train.copy())\n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest \n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 300, \n",
    "                                       random_state = 20231123, \n",
    "                                       n_jobs = 12\n",
    "                                       )\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest \n",
    "feature_importance_in_rf = rf_classifier.feature_importances_\n",
    "\n",
    "feature_importance_df_rf = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importance_in_rf\n",
    "})\n",
    "\n",
    "# 將 DataFrame 按照重要性排序\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by = 'Importance', ascending = False)\n",
    "\n",
    "# 繪製條形圖\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.bar(feature_importance_df_rf['Feature'], feature_importance_df_rf['Importance'], color = 'skyblue')\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(rotation=45, ha='right')  # 如果特徵名稱較多，可以旋轉 x 軸標籤\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定義參數空間\n",
    "# param_dist = {\n",
    "#     'learning_rate': uniform(0.01, 0.19),\n",
    "#     'n_estimators': randint(300, 1000),\n",
    "#     'max_depth': randint(3, 10),\n",
    "#     'min_child_weight': randint(1, 10),\n",
    "#     'gamma': uniform(0, 0.5),\n",
    "#     'subsample': uniform(0.6, 0.4),\n",
    "#     'colsample_bytree': uniform(0.6, 0.4),\n",
    "#     'reg_alpha': uniform(0, 1),\n",
    "#     'reg_lambda': uniform(0, 1),\n",
    "#     'scale_pos_weight': uniform(1, 5)\n",
    "# }\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(\n",
    "#     objective='binary:logistic',\n",
    "#     n_jobs=12,\n",
    "#     seed=20231121,\n",
    "#     enable_categorical=True,\n",
    "#     tree_method='hist'\n",
    "# )\n",
    "\n",
    "# # 使用隨機搜索進行參數調優\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     xgb_model, \n",
    "#     param_distributions=param_dist, \n",
    "#     n_iter=100, \n",
    "#     cv=5, \n",
    "#     verbose=1, \n",
    "#     n_jobs=-1, \n",
    "#     random_state=20231121,\n",
    "#     scoring='f1'\n",
    "# )\n",
    "\n",
    "# # 擬合模型\n",
    "# random_search.fit(X_train, y_train, \n",
    "#                   eval_set=[(X_test, y_test)], \n",
    "#                   early_stopping_rounds=50,\n",
    "#                   eval_metric='auc',\n",
    "#                   verbose=True)\n",
    "\n",
    "# # 獲取最佳模型\n",
    "# best_xgb_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義參數網格\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [300, 500, 700],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0, 0.1, 0.5],\n",
    "    'scale_pos_weight': [1, 10, 50]\n",
    "}\n",
    "\n",
    "# 定義基礎模型\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    n_jobs=12,\n",
    "    random_state=20231121,\n",
    "    enable_categorical=True,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "# 定義自定義評分函數\n",
    "def custom_score(y_true, y_pred):\n",
    "    return 0.5 * f1_score(y_true, y_pred) + 0.5 * roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# 使用網格搜索進行參數調優\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model, \n",
    "    param_grid=param_grid, \n",
    "    cv=5, \n",
    "    verbose=2,\n",
    "    n_jobs=-1, \n",
    "    scoring=make_scorer(custom_score),\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# 擬合模型\n",
    "grid_search.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    early_stopping_rounds=50,\n",
    "    eval_metric=['auc', 'aucpr'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 獲取最佳模型\n",
    "best_xgb_model_in_grid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance in 3  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "feature_importance_df_lgbm = pd.DataFrame({\n",
    "    'Feature': feature_names_in_lgbm,\n",
    "    'Importance': feature_importance_in_lgbm\n",
    "}).sort_values(by = 'Importance', ascending = True)\n",
    "\n",
    "# Random Forest\n",
    "feature_importance_df_rf = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importance_in_rf\n",
    "}).sort_values(by = 'Importance', ascending = True)\n",
    "\n",
    "# XGBoost (使用 plot_importance 函數的數據)\n",
    "fig_xgb, ax_xgb = plt.subplots(figsize=(10, 10))\n",
    "plot_importance(best_xgb_model_in_grid, ax = ax_xgb, importance_type = 'weight')\n",
    "plt.close(fig_xgb)  # 關閉 XGBoost 的單獨圖表\n",
    "\n",
    "# 從 ax_xgb \n",
    "xgb_importance = [patch.get_width() for patch in ax_xgb.patches]\n",
    "xgb_features = [text.get_text() for text in ax_xgb.get_yticklabels()]\n",
    "\n",
    "feature_importance_df_xgb = pd.DataFrame({\n",
    "    'Feature': xgb_features,\n",
    "    'Importance': xgb_importance\n",
    "}).sort_values(by = 'Importance', ascending = True)\n",
    "\n",
    "\n",
    "# Create plot\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (20, 6))\n",
    "fig.suptitle('Feature Importance Comparison', fontsize = 16)\n",
    "\n",
    "# LightGBM\n",
    "ax1.barh(feature_importance_df_lgbm['Feature'].tail(10), feature_importance_df_lgbm['Importance'].tail(10))\n",
    "ax1.set_title('LightGBM')\n",
    "ax1.set_xlabel('Importance')\n",
    "\n",
    "# Random Forest\n",
    "ax2.barh(feature_importance_df_rf['Feature'].tail(10), feature_importance_df_rf['Importance'].tail(10))\n",
    "ax2.set_title('Random Forest')\n",
    "ax2.set_xlabel('Importance')\n",
    "\n",
    "# XGBoost\n",
    "ax3.barh(feature_importance_df_xgb['Feature'].tail(10), feature_importance_df_xgb['Importance'].tail(10))\n",
    "ax3.set_title('XGBoost')\n",
    "ax3.set_xlabel('Importance')\n",
    "\n",
    "# adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top = 0.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probabilities_lgbm = bst.predict(X_test, num_iteration = bst.best_iteration)\n",
    "y_probabilities_rf = np.mean([estimator.predict_proba(X_test)[:, 1] for estimator in rf_classifier.estimators_], axis=0)\n",
    "y_probabilities_xgb = best_xgb_model_in_grid.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 3 model (lgbm + rf + xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(y_true, y_prob, thresholds):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "# 使用更精細的閾值範圍\n",
    "thresholds = np.arange(0.01, 1, 0.01)\n",
    "\n",
    "# 找到每個模型的最佳閾值\n",
    "best_threshold_lgbm, best_f1_lgbm = find_best_threshold(y_test.label, y_probabilities_lgbm, thresholds)\n",
    "best_threshold_xgb, best_f1_xgb = find_best_threshold(y_test.label, y_probabilities_xgb, thresholds)\n",
    "best_threshold_rf, best_f1_rf = find_best_threshold(y_test.label, y_probabilities_rf, thresholds)\n",
    "\n",
    "print(f\"LightGBM - Best Threshold: {best_threshold_lgbm:.2f}, Best F1: {best_f1_lgbm:.4f}\")\n",
    "print(f\"XGBoost - Best Threshold: {best_threshold_xgb:.2f}, Best F1: {best_f1_xgb:.4f}\")\n",
    "print(f\"Random Forest - Best Threshold: {best_threshold_rf:.2f}, Best F1: {best_f1_rf:.4f}\")\n",
    "\n",
    "# 計算並繪製各個閾值下的F1分數\n",
    "f1_scores_lgbm = [f1_score(y_test.label, (y_probabilities_lgbm >= t).astype(int)) for t in thresholds]\n",
    "f1_scores_xgb = [f1_score(y_test.label, (y_probabilities_xgb >= t).astype(int)) for t in thresholds]\n",
    "f1_scores_rf = [f1_score(y_test.label, (y_probabilities_rf >= t).astype(int)) for t in thresholds]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(thresholds, f1_scores_lgbm, label='LightGBM')\n",
    "plt.plot(thresholds, f1_scores_xgb, label='XGBoost')\n",
    "plt.plot(thresholds, f1_scores_rf, label='Random Forest')\n",
    "plt.axvline(x=best_threshold_lgbm, color='blue', linestyle='--', label='LightGBM Best')\n",
    "plt.axvline(x=best_threshold_xgb, color='orange', linestyle='--', label='XGBoost Best')\n",
    "plt.axvline(x=best_threshold_rf, color='green', linestyle='--', label='Random Forest Best')\n",
    "plt.title('F1 Score vs Threshold')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 計算並打印最佳閾值下的精確率和召回率\n",
    "precision_lgbm = precision_score(y_test.label, (y_probabilities_lgbm >= best_threshold_lgbm).astype(int))\n",
    "recall_lgbm = recall_score(y_test.label, (y_probabilities_lgbm >= best_threshold_lgbm).astype(int))\n",
    "\n",
    "precision_xgb = precision_score(y_test.label, (y_probabilities_xgb >= best_threshold_xgb).astype(int))\n",
    "recall_xgb = recall_score(y_test.label, (y_probabilities_xgb >= best_threshold_xgb).astype(int))\n",
    "\n",
    "precision_rf = precision_score(y_test.label, (y_probabilities_rf >= best_threshold_rf).astype(int))\n",
    "recall_rf = recall_score(y_test.label, (y_probabilities_rf >= best_threshold_rf).astype(int))\n",
    "\n",
    "print(f\"LightGBM - Precision: {precision_lgbm:.4f}, Recall: {recall_lgbm:.4f}\")\n",
    "print(f\"XGBoost - Precision: {precision_xgb:.4f}, Recall: {recall_xgb:.4f}\")\n",
    "print(f\"Random Forest - Precision: {precision_rf:.4f}, Recall: {recall_rf:.4f}\")\n",
    "\n",
    "# 找到一個共同的閾值\n",
    "common_thresholds = np.arange(0.3, 0.7, 0.01)  # 可以根據需要調整範圍\n",
    "f1_scores_common = []\n",
    "\n",
    "for threshold in common_thresholds:\n",
    "    f1_lgbm = f1_score(y_test.label, (y_probabilities_lgbm >= threshold).astype(int))\n",
    "    f1_xgb = f1_score(y_test.label, (y_probabilities_xgb >= threshold).astype(int))\n",
    "    f1_rf = f1_score(y_test.label, (y_probabilities_rf >= threshold).astype(int))\n",
    "    avg_f1 = (f1_lgbm + f1_xgb + f1_rf) / 3\n",
    "    f1_scores_common.append(avg_f1)\n",
    "\n",
    "best_common_threshold = common_thresholds[np.argmax(f1_scores_common)]\n",
    "print(f\"Best common threshold: {best_common_threshold:.2f}\")\n",
    "\n",
    "# 計算並打印使用共同閾值的結果\n",
    "for model_name, y_prob in [('LightGBM', y_probabilities_lgbm), \n",
    "                           ('XGBoost', y_probabilities_xgb), \n",
    "                           ('Random Forest', y_probabilities_rf)]:\n",
    "    y_pred = (y_prob >= best_common_threshold).astype(int)\n",
    "    precision = precision_score(y_test.label, y_pred)\n",
    "    recall = recall_score(y_test.label, y_pred)\n",
    "    f1 = f1_score(y_test.label, y_pred)\n",
    "    print(f\"{model_name} (Common Threshold) - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 2 model (lgbm & rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_best_threshold(y_true, y_prob, thresholds):\n",
    "#     best_f1 = 0\n",
    "#     best_threshold = 0\n",
    "#     for threshold in thresholds:\n",
    "#         y_pred = (y_prob >= threshold).astype(int)\n",
    "#         f1 = f1_score(y_true, y_pred)\n",
    "#         if f1 > best_f1:\n",
    "#             best_f1 = f1\n",
    "#             best_threshold = threshold\n",
    "#     return best_threshold, best_f1\n",
    "\n",
    "# # 使用更精細的閾值範圍\n",
    "# thresholds = np.arange(0.01, 1, 0.01)\n",
    "\n",
    "# # 找到每個模型的最佳閾值\n",
    "# best_threshold_lgbm, best_f1_lgbm = find_best_threshold(y_test.label, y_probabilities_lgbm, thresholds)\n",
    "# best_threshold_rf, best_f1_rf = find_best_threshold(y_test.label, y_probabilities_rf, thresholds)\n",
    "\n",
    "# print(f\"LightGBM - Best Threshold: {best_threshold_lgbm:.2f}, Best F1: {best_f1_lgbm:.4f}\")\n",
    "# print(f\"Random Forest - Best Threshold: {best_threshold_rf:.2f}, Best F1: {best_f1_rf:.4f}\")\n",
    "\n",
    "# # 計算並繪製各個閾值下的F1分數\n",
    "# f1_scores_lgbm = [f1_score(y_test.label, (y_probabilities_lgbm >= t).astype(int)) for t in thresholds]\n",
    "# f1_scores_rf = [f1_score(y_test.label, (y_probabilities_rf >= t).astype(int)) for t in thresholds]\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(thresholds, f1_scores_lgbm, label='LightGBM')\n",
    "# plt.plot(thresholds, f1_scores_rf, label='Random Forest')\n",
    "# plt.axvline(x=best_threshold_lgbm, color='blue', linestyle='--', label='LightGBM Best')\n",
    "# plt.axvline(x=best_threshold_rf, color='green', linestyle='--', label='Random Forest Best')\n",
    "# plt.title('F1 Score vs Threshold')\n",
    "# plt.xlabel('Threshold')\n",
    "# plt.ylabel('F1 Score')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # 計算並打印最佳閾值下的精確率和召回率\n",
    "# precision_lgbm = precision_score(y_test.label, (y_probabilities_lgbm >= best_threshold_lgbm).astype(int))\n",
    "# recall_lgbm = recall_score(y_test.label, (y_probabilities_lgbm >= best_threshold_lgbm).astype(int))\n",
    "\n",
    "# precision_rf = precision_score(y_test.label, (y_probabilities_rf >= best_threshold_rf).astype(int))\n",
    "# recall_rf = recall_score(y_test.label, (y_probabilities_rf >= best_threshold_rf).astype(int))\n",
    "\n",
    "# print(f\"LightGBM - Precision: {precision_lgbm}, Recall: {recall_lgbm}\")\n",
    "# print(f\"Random Forest - Precision: {precision_rf}, Recall: {recall_rf}\")\n",
    "\n",
    "# # 找到一個共同的閾值\n",
    "# common_thresholds = np.arange(0.3, 0.7, 0.01)  # 可以根據需要調整範圍\n",
    "# f1_scores_common = []\n",
    "\n",
    "# for threshold in common_thresholds:\n",
    "#     f1_lgbm = f1_score(y_test.label, (y_probabilities_lgbm >= threshold).astype(int))\n",
    "#     f1_rf = f1_score(y_test.label, (y_probabilities_rf >= threshold).astype(int))\n",
    "#     avg_f1 = (f1_lgbm + f1_rf) / 2\n",
    "#     f1_scores_common.append(avg_f1)\n",
    "\n",
    "# best_common_threshold = common_thresholds[np.argmax(f1_scores_common)]\n",
    "# print(f\"Best common threshold: {best_common_threshold:.2f}\")\n",
    "\n",
    "# # 計算並打印使用共同閾值的結果\n",
    "# for model_name, y_prob in [('LightGBM', y_probabilities_lgbm), \n",
    "#                            ('Random Forest', y_probabilities_rf)]:\n",
    "#     y_pred = (y_prob >= best_common_threshold).astype(int)\n",
    "#     precision = precision_score(y_test.label, y_pred)\n",
    "#     recall = recall_score(y_test.label, y_pred)\n",
    "#     f1 = f1_score(y_test.label, y_pred)\n",
    "#     print(f\"{model_name} (Common Threshold) - Precision: {precision}, Recall: {recall}, F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrated all the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated all the result\n",
    "y_probabilities_xgb_train = best_xgb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "y_probabilities_lgbm_train = bst.predict(X_test, \n",
    "                                         num_iteration = bst.best_iteration\n",
    "                                         )\n",
    "\n",
    "y_probabilities_rf_train = np.mean([estimator.predict_proba(X_test)[:, 1] for estimator in rf_classifier.estimators_], axis=0)\n",
    "\n",
    "y_probabilities = (y_probabilities_rf_train + y_probabilities_lgbm_train + y_probabilities_xgb_train) / 3\n",
    "\n",
    "\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.scatter(range(len(y_test)), \n",
    "            y_probabilities, \n",
    "            c =  y_test.label, \n",
    "            cmap = 'viridis', \n",
    "            alpha = 0.7\n",
    "            )\n",
    "\n",
    "plt.colorbar(label = 'True Label')\n",
    "plt.title('Scatter Plot of Probabilities')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.01, 1, 0.01)\n",
    "predictions = [(y_probabilities >= threshold).astype(int) for threshold in thresholds]\n",
    "precision_values = [precision_score(y_test.label, prediction) for prediction in predictions]\n",
    "recall_values = [recall_score(y_test.label, prediction) for prediction in predictions]\n",
    "f1_values = [f1_score(y_test.label, y_probabilities >= threshold) for threshold in thresholds]\n",
    "\n",
    "# 計算 ROC 曲線\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test.label, y_probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# 繪製 ROC 曲線\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(fpr, tpr, label = f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray', label='Random')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 繪製 precision-recall 曲線\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(recall_values, precision_values, label='Precision-Recall Curve')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 繪製 F1 score 曲線\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(thresholds, f1_values, label='F1 Score', color='orange')\n",
    "plt.title('F1 Score Curve')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The max F1 score of emsemble result is {max(f1_values)}\")\n",
    "print(thresholds[np.argmax(f1_values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probabilities_combined = (y_probabilities_rf + y_probabilities_lgbm + y_probabilities_xgb) / 3\n",
    "\n",
    "# 繪製散點圖\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "models = [('LightGBM', y_probabilities_lgbm), \n",
    "          ('XGBoost', y_probabilities_xgb), \n",
    "          ('Random Forest', y_probabilities_rf),\n",
    "          ('Combined', y_probabilities_combined)]\n",
    "\n",
    "for i, (model_name, y_prob) in enumerate(models):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    scatter = ax.scatter(range(len(y_test)), y_prob, c=y_test.label, cmap='viridis', alpha=0.7)\n",
    "    ax.set_title(f'Scatter Plot of Probabilities in {model_name}')\n",
    "    ax.set_xlabel('Sample Index')\n",
    "    ax.set_ylabel('Probability')\n",
    "    fig.colorbar(scatter, ax=ax, label='True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 計算評估指標的函數\n",
    "def calculate_metrics(y_true, y_prob, thresholds):\n",
    "    predictions = [(y_prob >= threshold).astype(int) for threshold in thresholds]\n",
    "    precision_values = [precision_score(y_true, prediction) for prediction in predictions]\n",
    "    recall_values = [recall_score(y_true, prediction) for prediction in predictions]\n",
    "    f1_values = [f1_score(y_true, prediction) for prediction in predictions]\n",
    "    return precision_values, recall_values, f1_values\n",
    "\n",
    "# 計算各模型的評估指標\n",
    "thresholds = np.arange(0.01, 1, 0.01)\n",
    "metrics = {}\n",
    "for model_name, y_prob in models:\n",
    "    metrics[model_name] = calculate_metrics(y_test.label, y_prob, thresholds)\n",
    "\n",
    "# 繪製 Precision-Recall 曲線\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model_name, (precision, recall, _) in metrics.items():\n",
    "    plt.plot(recall, precision, label=model_name)\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 繪製 F1 分數曲線\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model_name, (_, _, f1) in metrics.items():\n",
    "    plt.plot(thresholds, f1, label=model_name)\n",
    "plt.title('F1 Score vs Threshold')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 繪製 ROC 曲線\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model_name, y_prob in models:\n",
    "    fpr, tpr, _ = roc_curve(y_test.label, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray', label='Random')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction on testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import raw X_test_pub data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pub = pd.read_csv(\"X_test_pub.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probabilities_lgbm = bst.predict(X_test_pub, \n",
    "                                   num_iteration = bst.best_iteration\n",
    "                                   )\n",
    "\n",
    "y_probabilities_rf = np.mean([estimator.predict_proba(X_test_pub)[:, 1] for estimator in rf_classifier.estimators_], axis=0)\n",
    "\n",
    "y_probabilities_xgb = best_xgb_model.predict_proba(X_test_pub)[:,1]\n",
    "\n",
    "y_probabilities = (y_probabilities_lgbm + y_probabilities_rf + y_probabilities_xgb)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_test = pd.read_csv(\"X_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (y_probabilities >= 0.36).astype(int)\n",
    "df_raw_test['label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從原始測試數據中提取 'txkey' 和 'pred' 列\n",
    "df_out = df_raw_test[['txkey', 'label']]\n",
    "\n",
    "# 創建新的數據框，只包含所需的列\n",
    "df_final = pd.DataFrame({\n",
    "    'index': range(len(df_out)),  # 從 0 開始的序列\n",
    "    'label': df_out['label']  # 使用 'pred' 列的數據\n",
    "})\n",
    "\n",
    "# 設置數據框的索引從 1 開始\n",
    "# df_final.index = range(0, len(df_final))\n",
    "\n",
    "# 驗證結果\n",
    "print(df_final.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('./result/RF+LightGBM+XG_test7.csv', \n",
    "              index = False, \n",
    "              encoding = 'utf-8'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
